{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 12:50:01.914465: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-18 12:50:01.943288: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.callbacks\n",
    "import keras.optimizers\n",
    "import keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.loss_functions as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.image_loader import read_images, get_image_paths\n",
    "BASE_PATH = '/app/data/imagenet_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]2023-06-18 12:50:03.164011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "  0%|          | 1/2000 [00:00<21:29,  1.55it/s]2023-06-18 12:50:03.166998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.167088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.167643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.167723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.167796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.561120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.561245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.561321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-18 12:50:03.561385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22285 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "100%|██████████| 2000/2000 [00:11<00:00, 175.67it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 468.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_slice = 0,2000\n",
    "\n",
    "train_color_paths = get_image_paths(os.path.join(BASE_PATH, 'train/color'), *train_slice)\n",
    "train_color_imgs = read_images(train_color_paths, resize_dimensions=(400,400), show_progress=True)\n",
    "train_gray_paths  = get_image_paths(os.path.join(BASE_PATH, 'train/grayscale'),  *train_slice)\n",
    "train_gray_imgs  = read_images(train_gray_paths, resize_dimensions=(400,400), show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:04<00:00, 211.41it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 456.84it/s]\n"
     ]
    }
   ],
   "source": [
    "vali_slice = 100,1000\n",
    "\n",
    "vali_color_paths = get_image_paths(os.path.join(BASE_PATH, 'vali/color'), *vali_slice)\n",
    "vali_color_imgs = read_images(vali_color_paths, resize_dimensions=(400,400), show_progress=True)\n",
    "vali_gray_paths  = get_image_paths(os.path.join(BASE_PATH, 'vali/grayscale'),  *vali_slice)\n",
    "vali_gray_imgs  = read_images(vali_gray_paths, resize_dimensions=(400,400), show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 210.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 449.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_color_paths = get_image_paths(os.path.join(BASE_PATH, 'test/color'), 0,100)\n",
    "test_color_imgs = read_images(test_color_paths, resize_dimensions=(400,400), show_progress=True)\n",
    "test_gray_paths  = get_image_paths(os.path.join(BASE_PATH, 'test/grayscale'),  0,100)\n",
    "test_gray_imgs  = read_images(test_gray_paths, resize_dimensions=(400,400), show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 400, 400, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 400, 400, 64  640         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 200, 200, 64  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 200, 200, 64  256        ['max_pooling2d[0][0]']          \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 200, 200, 32  18464       ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 200, 200, 32  9248        ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 100, 100, 32  0          ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100, 100, 32  128        ['max_pooling2d_1[0][0]']        \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 100, 100, 32  9248        ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 100, 100, 32  9248        ['conv2d_3[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 32)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 50, 50, 32)  128         ['max_pooling2d_2[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 50, 50, 32)   9248        ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 100, 100, 32  9248       ['conv2d_5[0][0]']               \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 100, 100, 32  9248        ['conv2d_transpose[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 100, 100, 32  0           ['conv2d_6[0][0]',               \n",
      "                                )                                 'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 200, 200, 32  9248       ['add[0][0]']                    \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 200, 200, 32  128        ['conv2d_transpose_1[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 200, 200, 32  9248        ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 200, 200, 32  0           ['conv2d_7[0][0]',               \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 400, 400, 32  9248       ['add_1[0][0]']                  \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 400, 400, 32  128        ['conv2d_transpose_2[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 400, 400, 32  9248        ['batch_normalization_4[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 400, 400, 3)  867         ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 113,219\n",
      "Trainable params: 112,835\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dims = train_gray_imgs[0].shape\n",
    "\n",
    "input_layer = kl.Input(shape=input_dims)\n",
    "x = kl.Conv2D(64, (3,3), activation='relu', padding='same')(input_layer)\n",
    "x = kl.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = kl.BatchNormalization()(x)\n",
    "skip1 = kl.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = kl.Conv2D(32, (3,3), activation='relu', padding='same')(skip1)\n",
    "x = kl.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = kl.BatchNormalization()(x)\n",
    "skip2 = kl.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "x = kl.Conv2D(32, (3,3), activation='relu', padding='same')(skip2)\n",
    "x = kl.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = kl.BatchNormalization()(x)\n",
    "x = kl.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "\n",
    "# upscale \n",
    "x = kl.Conv2DTranspose(32, (3,3), strides=(2,2), activation='relu', padding='same')(x)\n",
    "x = kl.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "x = kl.Add()([x, skip2])\n",
    "x = kl.Conv2DTranspose(32, (3,3), strides=(2,2), activation='relu', padding='same')(x)\n",
    "x = kl.BatchNormalization()(x)\n",
    "x = kl.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "x = kl.Add()([x, skip1])\n",
    "x = kl.Conv2DTranspose(32, (3,3), strides=(2,2), activation='relu', padding='same')(x)\n",
    "x = kl.BatchNormalization()(x)\n",
    "x = kl.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "output_layer = kl.Conv2D(3, (3,3), activation='relu', padding='same')(x)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 12:50:34.741489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-18 12:50:36.661872: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661911: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661923: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661933: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661943: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661952: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661961: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661971: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661980: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:36.661989: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-06-18 12:50:37.531968: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n",
      "2023-06-18 12:50:49.052357: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.91GiB (rounded to 2048000000)requested by op gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-06-18 12:50:49.052442: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-06-18 12:50:49.052479: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 124, Chunks in use: 123. 31.0KiB allocated for chunks. 30.8KiB in use in bin. 12.6KiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052502: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052526: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052549: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 6, Chunks in use: 6. 17.2KiB allocated for chunks. 17.2KiB in use in bin. 16.9KiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052571: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 3.4KiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052590: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052610: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052633: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 30, Chunks in use: 30. 1.05MiB allocated for chunks. 1.05MiB in use in bin. 1.05MiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052662: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 216.0KiB allocated for chunks. 216.0KiB in use in bin. 216.0KiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052683: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052707: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052730: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052753: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052768: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 0. 2.34MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052781: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052794: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052810: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 3, Chunks in use: 3. 91.55MiB allocated for chunks. 91.55MiB in use in bin. 91.55MiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052827: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 2. 122.07MiB allocated for chunks. 122.07MiB in use in bin. 122.07MiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052843: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 8, Chunks in use: 7. 930.30MiB allocated for chunks. 854.49MiB in use in bin. 854.49MiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052859: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 183.11MiB allocated for chunks. 183.11MiB in use in bin. 183.11MiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052875: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 16, Chunks in use: 14. 20.46GiB allocated for chunks. 19.08GiB in use in bin. 19.07GiB client-requested in use in bin.\n",
      "2023-06-18 12:50:49.052889: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 1.91GiB was 256.00MiB, Chunk State: \n",
      "2023-06-18 12:50:49.052917: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 412.73MiB | Requested Size: 183.11MiB | in_use: 0 | bin_num: 20, prev:   Size: 122.07MiB | Requested Size: 122.07MiB | in_use: 1 | bin_num: -1, next:   Size: 122.07MiB | Requested Size: 122.07MiB | in_use: 1 | bin_num: -1\n",
      "2023-06-18 12:50:49.052943: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 1000.74MiB | Requested Size: 183.11MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.91GiB | Requested Size: 1.91GiB | in_use: 1 | bin_num: -1\n",
      "2023-06-18 12:50:49.052960: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 23367712768\n",
      "2023-06-18 12:50:49.052976: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000000 of size 256 next 3\n",
      "2023-06-18 12:50:49.052987: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000100 of size 256 next 4\n",
      "2023-06-18 12:50:49.052997: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000200 of size 256 next 6\n",
      "2023-06-18 12:50:49.053008: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000300 of size 256 next 7\n",
      "2023-06-18 12:50:49.053019: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000400 of size 256 next 5\n",
      "2023-06-18 12:50:49.053029: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000500 of size 256 next 8\n",
      "2023-06-18 12:50:49.053040: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000600 of size 256 next 11\n",
      "2023-06-18 12:50:49.053050: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000700 of size 256 next 171\n",
      "2023-06-18 12:50:49.053061: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000800 of size 256 next 13\n",
      "2023-06-18 12:50:49.053071: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000900 of size 256 next 14\n",
      "2023-06-18 12:50:49.053088: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000a00 of size 256 next 15\n",
      "2023-06-18 12:50:49.053099: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000b00 of size 256 next 18\n",
      "2023-06-18 12:50:49.053109: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000c00 of size 256 next 16\n",
      "2023-06-18 12:50:49.053120: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000d00 of size 256 next 17\n",
      "2023-06-18 12:50:49.053130: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000e00 of size 256 next 23\n",
      "2023-06-18 12:50:49.053141: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e000f00 of size 256 next 21\n",
      "2023-06-18 12:50:49.053153: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001000 of size 256 next 22\n",
      "2023-06-18 12:50:49.053170: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001100 of size 256 next 176\n",
      "2023-06-18 12:50:49.053188: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001200 of size 256 next 27\n",
      "2023-06-18 12:50:49.053206: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001300 of size 256 next 28\n",
      "2023-06-18 12:50:49.053225: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001400 of size 256 next 29\n",
      "2023-06-18 12:50:49.053245: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001500 of size 256 next 30\n",
      "2023-06-18 12:50:49.053264: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001600 of size 256 next 9\n",
      "2023-06-18 12:50:49.053285: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e001700 of size 2304 next 10\n",
      "2023-06-18 12:50:49.053305: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002000 of size 256 next 184\n",
      "2023-06-18 12:50:49.053326: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002100 of size 256 next 34\n",
      "2023-06-18 12:50:49.053347: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002200 of size 256 next 35\n",
      "2023-06-18 12:50:49.053367: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002300 of size 256 next 36\n",
      "2023-06-18 12:50:49.053388: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002400 of size 256 next 39\n",
      "2023-06-18 12:50:49.053408: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002500 of size 256 next 41\n",
      "2023-06-18 12:50:49.053427: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002600 of size 256 next 43\n",
      "2023-06-18 12:50:49.053448: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002700 of size 256 next 45\n",
      "2023-06-18 12:50:49.053468: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002800 of size 256 next 192\n",
      "2023-06-18 12:50:49.053488: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002900 of size 256 next 47\n",
      "2023-06-18 12:50:49.053508: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002a00 of size 256 next 48\n",
      "2023-06-18 12:50:49.053532: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002b00 of size 256 next 49\n",
      "2023-06-18 12:50:49.053552: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002c00 of size 256 next 51\n",
      "2023-06-18 12:50:49.053573: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002d00 of size 256 next 53\n",
      "2023-06-18 12:50:49.053593: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e002e00 of size 256 next 54\n",
      "2023-06-18 12:50:49.053612: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f6b0e002f00 of size 256 next 55\n",
      "2023-06-18 12:50:49.053631: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003000 of size 256 next 56\n",
      "2023-06-18 12:50:49.053651: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003100 of size 256 next 57\n",
      "2023-06-18 12:50:49.053672: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003200 of size 256 next 59\n",
      "2023-06-18 12:50:49.053691: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003300 of size 256 next 61\n",
      "2023-06-18 12:50:49.053711: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003400 of size 256 next 62\n",
      "2023-06-18 12:50:49.053731: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003500 of size 256 next 60\n",
      "2023-06-18 12:50:49.053751: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003600 of size 256 next 63\n",
      "2023-06-18 12:50:49.053770: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003700 of size 256 next 66\n",
      "2023-06-18 12:50:49.053790: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003800 of size 256 next 69\n",
      "2023-06-18 12:50:49.053822: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003900 of size 256 next 70\n",
      "2023-06-18 12:50:49.053844: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003a00 of size 256 next 71\n",
      "2023-06-18 12:50:49.053868: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003b00 of size 256 next 72\n",
      "2023-06-18 12:50:49.053889: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003c00 of size 256 next 73\n",
      "2023-06-18 12:50:49.053913: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003d00 of size 256 next 74\n",
      "2023-06-18 12:50:49.053935: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003e00 of size 256 next 75\n",
      "2023-06-18 12:50:49.053959: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e003f00 of size 256 next 76\n",
      "2023-06-18 12:50:49.053981: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004000 of size 2304 next 77\n",
      "2023-06-18 12:50:49.054004: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004900 of size 256 next 78\n",
      "2023-06-18 12:50:49.054026: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004a00 of size 256 next 79\n",
      "2023-06-18 12:50:49.054050: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004b00 of size 256 next 80\n",
      "2023-06-18 12:50:49.054071: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004c00 of size 256 next 82\n",
      "2023-06-18 12:50:49.054095: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004d00 of size 256 next 83\n",
      "2023-06-18 12:50:49.054116: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004e00 of size 256 next 84\n",
      "2023-06-18 12:50:49.054138: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e004f00 of size 256 next 85\n",
      "2023-06-18 12:50:49.054161: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e005000 of size 256 next 86\n",
      "2023-06-18 12:50:49.054184: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e005100 of size 256 next 64\n",
      "2023-06-18 12:50:49.054208: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e005200 of size 3584 next 65\n",
      "2023-06-18 12:50:49.054238: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006000 of size 256 next 88\n",
      "2023-06-18 12:50:49.054261: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006100 of size 256 next 89\n",
      "2023-06-18 12:50:49.054287: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006200 of size 256 next 91\n",
      "2023-06-18 12:50:49.054307: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006300 of size 256 next 93\n",
      "2023-06-18 12:50:49.054328: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006400 of size 256 next 95\n",
      "2023-06-18 12:50:49.054351: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006500 of size 256 next 97\n",
      "2023-06-18 12:50:49.054373: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006600 of size 256 next 98\n",
      "2023-06-18 12:50:49.054400: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006700 of size 256 next 99\n",
      "2023-06-18 12:50:49.054422: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006800 of size 256 next 101\n",
      "2023-06-18 12:50:49.054446: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006900 of size 256 next 103\n",
      "2023-06-18 12:50:49.054468: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006a00 of size 256 next 104\n",
      "2023-06-18 12:50:49.054492: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006b00 of size 256 next 105\n",
      "2023-06-18 12:50:49.054515: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006c00 of size 256 next 107\n",
      "2023-06-18 12:50:49.054539: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e006d00 of size 3584 next 108\n",
      "2023-06-18 12:50:49.054560: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e007b00 of size 256 next 109\n",
      "2023-06-18 12:50:49.054582: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e007c00 of size 2304 next 110\n",
      "2023-06-18 12:50:49.054608: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008500 of size 256 next 111\n",
      "2023-06-18 12:50:49.054631: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008600 of size 256 next 112\n",
      "2023-06-18 12:50:49.054654: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008700 of size 256 next 113\n",
      "2023-06-18 12:50:49.054676: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008800 of size 256 next 115\n",
      "2023-06-18 12:50:49.054700: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008900 of size 256 next 117\n",
      "2023-06-18 12:50:49.054721: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008a00 of size 256 next 118\n",
      "2023-06-18 12:50:49.054744: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008b00 of size 256 next 119\n",
      "2023-06-18 12:50:49.054767: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008c00 of size 256 next 121\n",
      "2023-06-18 12:50:49.054789: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008d00 of size 256 next 123\n",
      "2023-06-18 12:50:49.054812: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008e00 of size 256 next 124\n",
      "2023-06-18 12:50:49.054834: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e008f00 of size 256 next 125\n",
      "2023-06-18 12:50:49.054857: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009000 of size 256 next 127\n",
      "2023-06-18 12:50:49.054879: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009100 of size 256 next 129\n",
      "2023-06-18 12:50:49.054903: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009200 of size 256 next 131\n",
      "2023-06-18 12:50:49.054925: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009300 of size 256 next 133\n",
      "2023-06-18 12:50:49.054948: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009400 of size 256 next 134\n",
      "2023-06-18 12:50:49.054968: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009500 of size 256 next 135\n",
      "2023-06-18 12:50:49.054989: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009600 of size 256 next 137\n",
      "2023-06-18 12:50:49.055013: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009700 of size 256 next 139\n",
      "2023-06-18 12:50:49.055034: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009800 of size 256 next 140\n",
      "2023-06-18 12:50:49.055057: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009900 of size 256 next 141\n",
      "2023-06-18 12:50:49.055079: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009a00 of size 256 next 143\n",
      "2023-06-18 12:50:49.055101: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e009b00 of size 5376 next 31\n",
      "2023-06-18 12:50:49.055123: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e00b000 of size 36864 next 25\n",
      "2023-06-18 12:50:49.055147: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e014000 of size 36864 next 24\n",
      "2023-06-18 12:50:49.055169: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e01d000 of size 36864 next 20\n",
      "2023-06-18 12:50:49.055193: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e026000 of size 73728 next 19\n",
      "2023-06-18 12:50:49.055216: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e038000 of size 36864 next 32\n",
      "2023-06-18 12:50:49.055240: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e041000 of size 36864 next 38\n",
      "2023-06-18 12:50:49.055262: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e04a000 of size 36864 next 37\n",
      "2023-06-18 12:50:49.055284: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e053000 of size 36864 next 40\n",
      "2023-06-18 12:50:49.055307: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e05c000 of size 36864 next 42\n",
      "2023-06-18 12:50:49.055328: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e065000 of size 36864 next 44\n",
      "2023-06-18 12:50:49.055352: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e06e000 of size 36864 next 50\n",
      "2023-06-18 12:50:49.055374: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e077000 of size 36864 next 52\n",
      "2023-06-18 12:50:49.055398: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e080000 of size 36864 next 58\n",
      "2023-06-18 12:50:49.055420: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e089000 of size 73728 next 81\n",
      "2023-06-18 12:50:49.055443: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e09b000 of size 36864 next 87\n",
      "2023-06-18 12:50:49.055465: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0a4000 of size 36864 next 90\n",
      "2023-06-18 12:50:49.055489: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0ad000 of size 36864 next 92\n",
      "2023-06-18 12:50:49.055509: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0b6000 of size 36864 next 94\n",
      "2023-06-18 12:50:49.055530: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0bf000 of size 36864 next 96\n",
      "2023-06-18 12:50:49.055554: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0c8000 of size 36864 next 100\n",
      "2023-06-18 12:50:49.055575: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0d1000 of size 36864 next 102\n",
      "2023-06-18 12:50:49.055598: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0da000 of size 36864 next 106\n",
      "2023-06-18 12:50:49.055620: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0e3000 of size 73728 next 114\n",
      "2023-06-18 12:50:49.055644: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0f5000 of size 36864 next 116\n",
      "2023-06-18 12:50:49.055666: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e0fe000 of size 36864 next 120\n",
      "2023-06-18 12:50:49.055690: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e107000 of size 36864 next 122\n",
      "2023-06-18 12:50:49.055712: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e110000 of size 36864 next 126\n",
      "2023-06-18 12:50:49.055735: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e119000 of size 36864 next 128\n",
      "2023-06-18 12:50:49.055757: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e122000 of size 36864 next 130\n",
      "2023-06-18 12:50:49.055801: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e12b000 of size 36864 next 132\n",
      "2023-06-18 12:50:49.055824: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e134000 of size 36864 next 136\n",
      "2023-06-18 12:50:49.055845: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e13d000 of size 36864 next 138\n",
      "2023-06-18 12:50:49.055866: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e146000 of size 36864 next 142\n",
      "2023-06-18 12:50:49.055894: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f000 of size 256 next 144\n",
      "2023-06-18 12:50:49.055915: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f100 of size 256 next 145\n",
      "2023-06-18 12:50:49.055937: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f200 of size 256 next 146\n",
      "2023-06-18 12:50:49.055959: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f300 of size 256 next 147\n",
      "2023-06-18 12:50:49.055981: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f400 of size 256 next 148\n",
      "2023-06-18 12:50:49.056003: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f500 of size 256 next 149\n",
      "2023-06-18 12:50:49.056025: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f600 of size 256 next 150\n",
      "2023-06-18 12:50:49.056048: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f700 of size 256 next 151\n",
      "2023-06-18 12:50:49.056071: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f800 of size 256 next 152\n",
      "2023-06-18 12:50:49.056093: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14f900 of size 256 next 153\n",
      "2023-06-18 12:50:49.056116: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14fa00 of size 256 next 158\n",
      "2023-06-18 12:50:49.056137: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14fb00 of size 256 next 162\n",
      "2023-06-18 12:50:49.056161: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14fc00 of size 256 next 164\n",
      "2023-06-18 12:50:49.056183: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14fd00 of size 256 next 165\n",
      "2023-06-18 12:50:49.056209: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14fe00 of size 256 next 166\n",
      "2023-06-18 12:50:49.056232: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e14ff00 of size 256 next 167\n",
      "2023-06-18 12:50:49.056255: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150000 of size 256 next 172\n",
      "2023-06-18 12:50:49.056277: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150100 of size 256 next 173\n",
      "2023-06-18 12:50:49.056300: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150200 of size 256 next 177\n",
      "2023-06-18 12:50:49.056322: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150300 of size 256 next 178\n",
      "2023-06-18 12:50:49.056345: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150400 of size 256 next 33\n",
      "2023-06-18 12:50:49.056368: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150500 of size 256 next 187\n",
      "2023-06-18 12:50:49.056391: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150600 of size 256 next 189\n",
      "2023-06-18 12:50:49.056412: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150700 of size 256 next 194\n",
      "2023-06-18 12:50:49.056434: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e150800 of size 3584 next 195\n",
      "2023-06-18 12:50:49.056462: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f6b0e151600 of size 2458112 next 1\n",
      "2023-06-18 12:50:49.056485: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e3a9800 of size 1280 next 2\n",
      "2023-06-18 12:50:49.056510: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b0e3a9d00 of size 1280000000 next 67\n",
      "2023-06-18 12:50:49.056533: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6b5a85dd00 of size 3840000000 next 68\n",
      "2023-06-18 12:50:49.056558: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6c3f679d00 of size 64000000 next 154\n",
      "2023-06-18 12:50:49.056582: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6c43382d00 of size 128000000 next 155\n",
      "2023-06-18 12:50:49.056608: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6c4ad94d00 of size 128000000 next 156\n",
      "2023-06-18 12:50:49.056631: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6c527a6d00 of size 4096000000 next 157\n",
      "2023-06-18 12:50:49.056656: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6d469e6d00 of size 1024000000 next 161\n",
      "2023-06-18 12:50:49.056717: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6d83a76d00 of size 1024000000 next 163\n",
      "2023-06-18 12:50:49.056746: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6dc0b06d00 of size 512000000 next 12\n",
      "2023-06-18 12:50:49.056766: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6ddf34ed00 of size 512000000 next 168\n",
      "2023-06-18 12:50:49.056786: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6dfdb96d00 of size 128000000 next 169\n",
      "2023-06-18 12:50:49.056807: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e055a8d00 of size 192000000 next 170\n",
      "2023-06-18 12:50:49.056827: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e10cc3d00 of size 128000000 next 26\n",
      "2023-06-18 12:50:49.056849: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e186d5d00 of size 32000000 next 174\n",
      "2023-06-18 12:50:49.056872: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e1a55a500 of size 32000000 next 175\n",
      "2023-06-18 12:50:49.056897: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e1c3ded00 of size 32000000 next 179\n",
      "2023-06-18 12:50:49.056918: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e1e263500 of size 128000000 next 180\n",
      "2023-06-18 12:50:49.056939: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e25c75500 of size 128000000 next 181\n",
      "2023-06-18 12:50:49.056961: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f6e2d687500 of size 432777216 next 160\n",
      "2023-06-18 12:50:49.056983: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e47341d00 of size 128000000 next 182\n",
      "2023-06-18 12:50:49.057002: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e4ed53d00 of size 512000000 next 183\n",
      "2023-06-18 12:50:49.057021: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e6d59bd00 of size 512000000 next 188\n",
      "2023-06-18 12:50:49.057040: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6e8bde3d00 of size 512000000 next 46\n",
      "2023-06-18 12:50:49.057060: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6eaa62bd00 of size 64000000 next 159\n",
      "2023-06-18 12:50:49.057079: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f6eae334d00 of size 79489792 next 185\n",
      "2023-06-18 12:50:49.057101: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6eb2f03800 of size 512000000 next 186\n",
      "2023-06-18 12:50:49.057121: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6ed174b800 of size 2048000000 next 190\n",
      "2023-06-18 12:50:49.057142: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6f4b86b800 of size 2058252800 next 191\n",
      "2023-06-18 12:50:49.057163: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f6fc6352a00 of size 2048000000 next 193\n",
      "2023-06-18 12:50:49.057184: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f7040472a00 of size 1049351680 next 18446744073709551615\n",
      "2023-06-18 12:50:49.057204: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-06-18 12:50:49.057233: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 123 Chunks of size 256 totalling 30.8KiB\n",
      "2023-06-18 12:50:49.057257: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-06-18 12:50:49.057284: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 2304 totalling 6.8KiB\n",
      "2023-06-18 12:50:49.057307: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 3584 totalling 10.5KiB\n",
      "2023-06-18 12:50:49.057328: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 5376 totalling 5.2KiB\n",
      "2023-06-18 12:50:49.057352: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 30 Chunks of size 36864 totalling 1.05MiB\n",
      "2023-06-18 12:50:49.057375: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 73728 totalling 216.0KiB\n",
      "2023-06-18 12:50:49.057401: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 32000000 totalling 91.55MiB\n",
      "2023-06-18 12:50:49.057425: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 64000000 totalling 122.07MiB\n",
      "2023-06-18 12:50:49.057449: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 7 Chunks of size 128000000 totalling 854.49MiB\n",
      "2023-06-18 12:50:49.057472: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 192000000 totalling 183.11MiB\n",
      "2023-06-18 12:50:49.057496: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 6 Chunks of size 512000000 totalling 2.86GiB\n",
      "2023-06-18 12:50:49.057519: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1024000000 totalling 1.91GiB\n",
      "2023-06-18 12:50:49.057544: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280000000 totalling 1.19GiB\n",
      "2023-06-18 12:50:49.057566: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2048000000 totalling 3.81GiB\n",
      "2023-06-18 12:50:49.057591: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2058252800 totalling 1.92GiB\n",
      "2023-06-18 12:50:49.057614: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3840000000 totalling 3.58GiB\n",
      "2023-06-18 12:50:49.057639: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4096000000 totalling 3.81GiB\n",
      "2023-06-18 12:50:49.057661: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 20.31GiB\n",
      "2023-06-18 12:50:49.057688: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 23367712768 memory_limit_: 23367712768 available bytes: 0 curr_region_allocation_bytes_: 46735425536\n",
      "2023-06-18 12:50:49.057724: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                     23367712768\n",
      "InUse:                     21803635712\n",
      "MaxInUse:                  22398930944\n",
      "NumAllocs:                        6506\n",
      "MaxAllocSize:               4176779776\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-06-18 12:50:49.057770: W tensorflow/tsl/framework/bfc_allocator.cc:497] **********************************************************_*************************************____\n",
      "2023-06-18 12:50:49.057823: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_grad_input_ops.h:367 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[100,32,400,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-06-18 12:50:49.057876: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[100,32,400,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_13/3606337766.py\", line 1, in <module>\n      hist = model.fit(train_gray_imgs, train_color_imgs, epochs=10, batch_size=100, validation_data=(vali_gray_imgs, vali_color_imgs), callbacks=[early_stopping])\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/legacy/optimizer_v2.py\", line 585, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/legacy/optimizer_v2.py\", line 643, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/legacy/optimizer_v2.py\", line 519, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput'\nOOM when allocating tensor with shape[100,32,400,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_8388]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_gray_imgs, train_color_imgs, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(vali_gray_imgs, vali_color_imgs), callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_13/3606337766.py\", line 1, in <module>\n      hist = model.fit(train_gray_imgs, train_color_imgs, epochs=10, batch_size=100, validation_data=(vali_gray_imgs, vali_color_imgs), callbacks=[early_stopping])\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/legacy/optimizer_v2.py\", line 585, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/legacy/optimizer_v2.py\", line 643, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/legacy/optimizer_v2.py\", line 519, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput'\nOOM when allocating tensor with shape[100,32,400,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_8388]"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_gray_imgs, train_color_imgs, epochs=10, batch_size=100, validation_data=(vali_gray_imgs, vali_color_imgs), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = pd.DataFrame(hist.history)\n",
    "df_hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array((model.predict(test_gray_imgs) * 255), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all predictions in grid\n",
    "fig, axs = plt.subplots(10, 10, figsize=(20,20))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axs[i,j].imshow(preds[i*10+j].astype(int))\n",
    "        axs[i,j].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
